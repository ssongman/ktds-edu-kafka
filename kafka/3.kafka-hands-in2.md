# Kafka Hands-in 2

>Kafka On Kubernetes










# 1. 테스트 환경 준비





## 1) 테스트 환경

이전에 python Docker Container 으로 실습했던 자료를 참고하여 테스트 수행한다.



### [참고] Python Docker Container

```sh
# bastion Server 에서
## docker 실행이 안되었을 경우만 ....
$ docker run --name python --user root -d python:3.9 sleep 365d


# python 확인
$ docker ps -a
CONTAINER ID  IMAGE                         COMMAND     CREATED        STATUS            PORTS       NAMES
fb231e23f9f1  docker.io/library/python:3.9  sleep 365d  2 seconds ago  Up 2 seconds ago              python


# 1) python Container 내부로 진입( bash 명령 수행)
$ docker exec -it python bash
root@a225dc4c3dd7:/#           <-- 이런 prompt 가 표기 되어야 정상

# 2) 관련 library 설치
# python Conatiner 내부에서
$ pip install kafka-python

```



### Python 실습자료 download

python Conatiner에서 실습 자료를 download 하자.

```sh
# 1) 실습자료 download
# python Conatiner 내부에서
$ mkdir -p ~/githubrepo
  cd ~/githubrepo

$ git clone https://github.com/ssongman/ktds-edu-kafka.git

$ cd ~/githubrepo/ktds-edu-kafka

# 필요한경우 pull 을 다시 받는다.
$ git pull


```



### library & tool 설치

python Conatiner에서 실습 자료를 download 하자.

```sh
# python Conatiner 내부에서

# 1) 관련 library 설치
$ pip install configparser


# 2) vim 설치
$ apt update
$ apt install vim -y

```








# 2. Rebalancing Round

 일반적으로 Consumer group의 멤버 구성에 변화가 생기면 리소스의 재분배가 필요한데 이를 **Rebalancing Round** 라고 한다.

이 Rebalancing Round가 발생하는 동안은 어떤 컨슈머들도 정상적인 데이터 처리를 하지 못한다는 문제를 가지는데 카프카에서는 이를 **Stop The World** 라는 용어로 부른다. 

이는 수천 개의 Connect Task 가 그룹에 존재했을 때 그 수천 개의 프로세스가 전부 정상 동작하지 못하게 되는 상황을 맞이한다. 

이런 Rebalancing 과 관련된 Stop The World 는 일반적인 하드웨어나 네트워크 손실 문제로 발생한 일시적인 client fail 과 더불어, scale up / down 의 상황이나 계획적인 클라이언트 start / stop / restart 의 상황에서 전부 발생할 수 있다. 

그럼 Consumer 갯수에 따른 Partition 매핑 관계와 Consumer Rebalancing 현상에 대해서 확인해 보자.



## 1) Rebalancing 시나리오

### (1) 설명 

- Partition 3개인 Topic 에서 Consumer 갯수 변화에 따른 partition assigned 현황 관찰

### (2) Consumer 환경 

- Consumer1:  Spring boot

- Consumer2:  python [Container]
- Consumer3:  python [Container]
- Consumer4:  python [Container]

### (3) Producer 환경 

- 한개의 python producer 에서 1초에 한번씩 전송





## 2) 테스트 수행 

bastion Server Terminal 4개를 준비한다.  Mobaxterm Split 기능을 이용하면 4개의 terminal을 한꺼번에 볼수 있으니 참고하자.

4개의 모든 Termminal 에서는 docker 로 python container 로 진입한다.

![image-20240225120948131](3.kafka-hands-in2.assets/image-20240225120948131.png)





### (1) step1 :  Spring boot  - Consumer

- edu-topic__ / edu-group__ 셋팅후 실행

- Spring boot 로 Consumer 실행한다.

- Spring boot log 를 확인한다.

- 예상결과 : partition 1,2,3 이 모두 한꺼번에 assigned 될 것이다.

  ```
  edu-topic01-cg: partitions assigned: [edu-topic01-0, edu-topic01-1, edu-topic01-2]
  ```

  

### (2) step2 :  python  - Consumer1 실행

- Bastion Server Terminal 1번 접속

- python  Consumer 실행한다.

  - ```sh
    # 자신의 topic 명을 아규먼트로 입력한다.
    $ python ~/githubrepo/ktds-edu-kafka/kafka/PythonSample/11.consumer.py
    ```

    

- Spring boot log 를 확인한다.

- 예상결과 : partition 1,2,3 중 하나가 제외 처리 될 것이다.

  ```
  edu-topic01-cg: partitions assigned: [edu-topic01-0, edu-topic01-1]
  ```






### (3) step3 :  python  - Consumer2 실행

- Bastion Server Terminal 2번 접속

- python  Consumer 실행한다.

  - ```sh
    # 자신의 topic 명을 아규먼트로 입력한다.
    $ python ~/githubrepo/ktds-edu-kafka/kafka/PythonSample/11.consumer.py
    ```

    

- Spring boot log 를 확인한다.

- 예상결과 : partition 1,2,3 중 두개가 제외 처리 될 것이다.

  ```
  edu-topic01-cg: partitions assigned: [edu-topic01-0]
  ```



### (4) step4 :  python  - Consumer3 실행

- Bastion Server Terminal 3번 접속

- python  Consumer 실행한다.

  - ```sh
    # 자신의 topic 명을 아규먼트로 입력한다.
    $ python ~/githubrepo/ktds-edu-kafka/kafka/PythonSample/11.consumer.py
    ```

    

- Spring boot log 를 확인한다.

- 예상결과 : 변화사항이 없다. - assigned 될 partition 이 없어서 Consumer 가 낭비되는 상황이다.



### (5) step5 :  python  - Producer 실행

- Bastion Server Terminal 4번 접속

- python producer 초당 1회 발송

  - ```sh
    # 자신의 topic 명을 아규먼트로 입력한다.
    $ python ~/githubrepo/ktds-edu-kafka/kafka/PythonSample/12.producer.py
    ```

    

- Grafana 확인

- 각각 Consumer log 를 확인한다.

- 예상결과

  - 3개의 Consumer 에서 각각 로그가 균등하게 출력될 것이다. 
  - assigned 되지 못한 python-consumer3 은 변화가 없을 것이다.



### (6) step6 :  python  - Consumer 삭제

- producer 초당 1회 발송을 유지하며 ...
- python Consumer 3번을 제거한다. ( Ctrl+C  )
- [stop the world확인] 각각 Consumer log 를 확인한다. 
- python Consumer 2번을 제거 ( Ctrl+C  )
- [stop the world확인] 각각 Consumer log 를 확인한다. 
- python Consumer 1번을 제거 ( Ctrl+C  )
- [stop the world확인] 각각 Consumer log 를 확인한다. 

 

### (7) [참고] Clean up

```sh
# python 종료
#  Ctrl+C 로 python 종료

# Bastions Server Container 종료
$ docker ps -a

$ docker -rf python


# STS 실행종료
Stop 버튼으로 종료처리 수행

```







## 3) Trouble Shooting

### (1) 지속적인 Rebalancing 현상

#### 현상

Rebalancing 이 반복해서 발생하면서 처리가 멈추는 현상이 발생



#### 분석 - Rebalancing 발생하는 현상

Consumer 의 갯수가 변경 되는 경우 외에도 아래와 같은 경우가 발생할 수 있다.

* session.timeout.ms 설정시간(기본10초)에 heartbeat 시그널을 받지 못해 리밸런스가 발생하는 경우

* max.poll.interval.ms 설정시간(기본5분)에 poll() 메소드가 호출되지 않아 리밸런스가 발생하는 경우



| 옵션명                    | 설명                                                         | 기본값       |
| :------------------------ | :----------------------------------------------------------- | :----------- |
| `session.timeout.ms`      | 컨슈머와 브로커사이의 session timeout 시간. 컨슈머가 살아있는것으로 판단하는 시간으로 **이 시간이 지나면 해당 컨슈머는 종료되거나 장애가 발생한것으로 판단하고 컨슈머 그룹은 리밸런스를 시도한다.** 이 옵션은 heartbeat 없이 얼마나 오랫동안 컨슈머가 있을 수 있는지를 제어하며 heartbeat.interval.ms와 밀접한 관련이 있어서 일반적으로 두 속성이 함께 수정된다. | 10000 (10초) |
| `heartbeat.interval.ms`   | 컨슈머가 얼마나 자주 heartbeat을 보낼지 조정한다. session.timeout.ms보다 작아야 하며 일반적으로 1/3로 설정 | 3000 (3초)   |
| `max.poll.interval.ms`    | 컨슈머가 polling하고 commit 할때까지의 대기시간. 컨슈머가 살아있는지를 체크하기 위해 hearbeat를 주기적으로 보내는데, 계속해서 heartbeat만 보내고 실제로 메시지를 가져가지 않는 경우가 있을 수 있다. 이러한 경우에 컨슈머가 무한정 해당 파티션을 점유할 수 없도록 **주기적으로 poll을 호출하지 않으면 장애라고 판단하고 컨슈머 그룹에서 제외**시키도록 하는 옵션이다. | 300000 (5분) |
| `max.poll.records`        | 컨슈머가 최대로 가져 갈 수있는 갯수. 이 옵션으로 polling loop에서 데이터 양을 조정 할 수 있다. | 500          |
| `enable.auto.commit`      | 백그라운드로 주기적으로 offset을 commit                      | true         |
| `auto.commit.interval.ms` | 주기적으로 offset을 커밋하는 시간                            | 5000 (5초)   |
| `auto.offset.reset`       | earliest: 가장 초기의 offset값으로 설정 latest: 가장 마지막의 offset값으로 설정 none: 이전 offset값을 찾지 못하면 error 발생 | latest       |

[더 많은 컨슈머 옵션보기](https://kafka.apache.org/documentation/#consumerconfigs)



컨슈머는 메시지를 가져오기위해 브로커에 poll()요청을 보내고, 컨슈머는 가져온 메시지를 처리한 후, 해당 파티션의 offset을 커밋하게 된다.

poll요청을 보내고 다음 poll을 요청을 보내는데 까지의 시간이 max.poll.interval.ms의 기본값인 300000 (5분) 보다 늦으면 브로커는 컨슈머에 문제가 있다고 판단하여 리밸런싱을 일으키게 된다.

*max.poll.interval.ms 기본값 : 300000(5분)*
*max.poll.records 기본값 : 500*



그러므로 해결방안은 아래와 같이 두가지가 있을 수 있다.

1) 500개  처리하는데 소요되는 적정 시간을 늘린다.(ex, max.poll.interval.ms=600000 (10분))
2) 한번에 처리되는 poll 갯수를 줄인다. (ex,  max.poll.records=100 으로 조정)



#### [참고] java max.poll.interval.ms 조정 처리 예제

```java
public class ChangeKafkaListener
{	
	@KafkaListener( topics       = "my-topic"
				  , groupId      = "my-topic-group"
				  , errorHandler = "changeKafkaListenerErrorHandler"
				  , properties   = {"max.poll.interval.ms=108000000"} ) // 300000:5분, 108000000: 30분
	public void onMessage(String msg)
	{
		...
    }
```











# 3.  Producer 관련 실습



## 1) key 와 partition 관계 이해

- key 존재여부에 따라서 데이터가 어떻게 흘러가는지 확인해 보자.
- 기본적으로 kafka는 key가 설정되지 않은경우, 메시지는 reound robin 방식으로 파티션을 선택하여 메시지가 전달된다.

- key가 있다면 key값을 hashing하고 해싱 결과를 이용하여 파티션을 선택하도록 하고 있다.
- 만약 특정 메시지의 키에 따라 들어온 순서가 중요한 서비스라면, 키를 반드시 할당하고, 이들이 특정 파티션으로 분배되도록 지정해주는 용도로 사용할 수 있다.



### (1) 실습 환경



bastion 터미널 2개를 준비하자.

위는 ubuntu termnial, 아래는 python docker container terminal 이다.

![image-20240225125835013](3.kafka-hands-in2.assets/image-20240225125835013.png)



### (2) key 가 없는 경우 테스트

#### 테스트용 topic 생성

파티션 1개 topic 을 생성한다.

```sh
# ubuntu terminal 에서

$ ~/githubrepo/ktds-edu-kafka

$ cat ./kafka/strimzi/topic/12.kafka-edu-topic.yaml

apiVersion: kafka.strimzi.io/v1beta2
kind: KafkaTopic
metadata:
  name: edu-topic01-a
  labels:
    strimzi.io/cluster: my-cluster
  namespace: kafka
spec:
  partitions: 1
  replicas: 1
  config:
    #retention.ms: 7200000      # 2 hour
    retention.ms: 86400000      # 24 hours
    segment.bytes: 1073741824   # 1GB

# 본인의 topic 명으로 변경한다.
# 예를들면 
# edu-topic01-a --> edu-topic02-a
$ vi ./kafka/strimzi/topic/12.kafka-edu-topic.yaml
...


# 확인
$ cat ./kafka/strimzi/topic/12.kafka-edu-topic.yaml
...

# topic 생성
$ kubectl -n kafka apply -f ./kafka/strimzi/topic/12.kafka-edu-topic.yaml


# 확인
$ kubectl -n kafka get kafkatopic
NAME            CLUSTER      PARTITIONS   REPLICATION FACTOR   READY
edu-topic01     my-cluster   3            2                    True
edu-topic01-a   my-cluster   1            1                    True
edu-topic02     my-cluster   3            2                    True
edu-topic03     my-cluster   3            2                    True
...

```





#### kafkadrop 확인

http://kafdrop.kafka.43.203.62.69.nip.io/



#### 한번씩 10개 send

```sh
# python Container 에서

$ cd ~/githubrepo/ktds-edu-kafka

$ cat ./kafka/PythonSample/21.producer-10pub.py

# 10 회전송
$ python ./kafka/PythonSample/21.producer-10pub.py

```





### (3) partition 추가

#### topic에 partition 추가

파티션 2개 로 수정한다.

```sh
# ubuntu terminal 에서

$ ~/githubrepo/ktds-edu-kafka

# partitions: 2 로 수정
$ vi ./kafka/strimzi/topic/12.kafka-edu-topic.yaml

apiVersion: kafka.strimzi.io/v1beta2
kind: KafkaTopic
metadata:
  name: edu-topic01-a
  labels:
    strimzi.io/cluster: my-cluster
  namespace: kafka
spec:
  partitions: 2               # <--- 2로 수정한다.
  replicas: 1
...
---

# 확인
$ cat ./kafka/strimzi/topic/12.kafka-edu-topic.yaml
...



# topic 적용
$ kubectl -n kafka apply -f ./kafka/strimzi/topic/12.kafka-edu-topic.yaml


# 확인
$ kubectl -n kafka get kafkatopic
NAME            CLUSTER      PARTITIONS   REPLICATION FACTOR   READY
edu-topic01     my-cluster   3            2                    True
edu-topic01-a   my-cluster   2            1                    True  # <-- 변경되었음
edu-topic02     my-cluster   3            2                    True
...

```





#### kafkadrop 확인

http://kafdrop.kafka.43.203.62.69.nip.io/



#### 한번씩 10개 send

```sh
# python Container 에서

$ cd ~/githubrepo/ktds-edu-kafka

$ cat ./kafka/PythonSample/21.producer-10pub.py

# 10 회전송
$ python ./kafka/PythonSample/21.producer-10pub.py

```



#### 확인

kafkadrop을 통해서 확인해보자.

2개 파티션에 균등하게 RR방식으로 분산되어 있다.



### (4) key 를 지정 send

key가 존재하는 데이터를 토픽에 보낸다면 어떻게 될까?

key가 존해할경우 kafka 는 key 를 특정한 hash 값으로 변형시켜서 파티션과 1대1 매칭을 시킨다.

그러므로 각 파티션에 동일한 key  값만 쌓이게 된다.

kafka 메시지에 key를 할당하고, 이 key에 따라 파티션이 선택되도록 해보자.

```sh
# python Container 에서

$ cd ~/githubrepo/ktds-edu-kafka

# 소스 확인
$ cat ./kafka/PythonSample/22.producer-10pub_key.py


# key할당하여 메세지 송신 - key는 아규먼트로 입력
$ python ./kafka/PythonSample/22.producer-10pub_key.py key1


# kafkadrop 을 확인하면서 다시 한번 보내보자.

# 한번더 송신
$ python ./kafka/PythonSample/22.producer-10pub_key.py key1

```



#### 확인

kafkadrop을 통해서 확인해보면 1개 파티션에만 보내지는것으 확인할 수 있다.



다른 이름의 key 로 보내보자. 

```sh
# python Container 에서

$ cd ~/githubrepo/ktds-edu-kafka

$ cat ./kafka/PythonSample/22.producer-10pub_key.py

# key명을 아규먼트로 입력
$ python ./kafka/PythonSample/22.producer-10pub_key.py key2
$ python ./kafka/PythonSample/22.producer-10pub_key.py key3

```



### (5) key사용후 partition 추가

파티션을 한개 더 추가 할경우 어떻게 될까?

토픽에 파티션을 추가하는 순간 키와 파티션의 일관성은 보장되지 않는다.

그러므로 key 를 사용할 경우  추후 생성하지 않는 것을 권장한다.



#### topic에 partition 추가

파티션 3개 로 수정한다.

```sh
# ubuntu terminal 에서

$ ~/githubrepo/ktds-edu-kafka

# partitions: 2 로 수정
$ vi ./kafka/strimzi/topic/12.kafka-edu-topic.yaml

apiVersion: kafka.strimzi.io/v1beta2
kind: KafkaTopic
metadata:
  name: edu-topic01-a
  labels:
    strimzi.io/cluster: my-cluster
  namespace: kafka
spec:
  partitions: 3               # 2 --> 3로 수정한다.
  replicas: 1
...
---

# 확인
$ cat ./kafka/strimzi/topic/12.kafka-edu-topic.yaml
...



# topic 적용
$ kubectl -n kafka apply -f ./kafka/strimzi/topic/12.kafka-edu-topic.yaml


# 확인
$ kubectl -n kafka get kafkatopic
NAME            CLUSTER      PARTITIONS   REPLICATION FACTOR   READY
edu-topic01     my-cluster   3            2                    True
edu-topic01-a   my-cluster   3            1                    True  # <-- 변경되었음
edu-topic02     my-cluster   3            2                    True
...

```



#### data send

```sh
# python Container 에서

$ cd ~/githubrepo/ktds-edu-kafka

# 소스 확인
$ cat ./kafka/PythonSample/22.producer-10pub_key.py


# key할당하여 메세지 송신 - key는 아규먼트로 입력
$ python ./kafka/PythonSample/22.producer-10pub_key.py key1
$ python ./kafka/PythonSample/22.producer-10pub_key.py key2
$ python ./kafka/PythonSample/22.producer-10pub_key.py key3

```



#### 확인

kafkadrop을 통해서 확인해보면

파티션이 변경된 것을 확인할 수 있다.

토픽에 파티션을 추가하는 순간 키와 파티션의 일관성은 보장되지 않는다.

그러므로 key 를 사용할 경우  partition 생성하지 않는 것을 고려하거나 이런 사실에 맞도록 설계해야 할 것이다.









## 2) 전송 보장과 ack 

ack 값에 따라서 속도와 데이터 신뢰성이 달라지는 부분이 있다.

얼마만큼 속도가 차이나는지 확인해보자.



### (1) 기본정보

* ack = 0
  * 서버 응답을 기다리지않음
  * 전송 보장도 zero
* ack = 1
  * 파티션의 리더에 저장되면 응답 받음
  * 리더 장애시 메시지 유실 가능
* ack = all(-1)
  * 모든 리플리카에 저장되면 응답 받음
    * 브로커 min.insync.replicas 설정에 따라 달라짐



### (2) 실습환경









### (2) 옵션에 따른 속도체크

* 리플리카 갯수 3, ack=0
  * 서버 응답을 기다리지 않음
* 리플리카 갯수 3, ack=1
  * 리더가 저장되면 성공 응답
* 리플리카 갯수 3, ack=all, min.insync.replicas = 1
  * 리더에 저장되면 성공 응답
  * ack=1과 동일(리더 장애시 메시지 유실 가능)
* 리플리카 갯수 3, ack=all, min.insync.replicas = 2
  * 리더에 저장하고 팔로워 중 한개에 저장하면 성공 응답
* 리플리카 갯수 3, ack=all, min.insync.replicas = 3
  * 리더와 팔로워 2개에 저장되면 성공 응답
  * 팔로워 중 한개라도 장애가 나면 리플리카 부족으로 저장에 실패함
  * 그러므로 이렇게 설정하는 것은 가능한 지양해야 함







## 3) Sender 동작

### (1) 기본동작

```
Send() --> Serializer --> Partitioner --> 버퍼(배치) --> Sender ==> 카프카브로커


```

* Sender
  * 별도 쓰레드로 Sender 가 동작한다.



### (2) batch.size linger.ms 에 따른 메세지 전송차이



* batch.size
  * 배치크기, 배치가 다 차면 바로 전송
  * 사이즈가 너무 작으면 한번에 보낼 수 있는 메시지의 건수가 줄고 처리량이 떨어진다.

* linger.ms
  * 전송대기시간(기본값 0)
  * 대기시간이 없으면 배치가 덜 차도 브로커로 바로 전송
  * 대기시간을 주면 그 시간 만큼 배치에 메시지 추가가 가능해서 한번의 전송 요청에 더 많은 데이터 처리가능
    * 처리량이 높아진다.





## 4) 전송결과 확인



### (1) future 이용

* Send method 가 리턴하는 future 를 활용해서 알수 있다.

* ```java
  Future<RecordMetadata> f = producer.send(new ProducerRecord<>("topic", "value"));
  try{
     RecordMetadata meta = f.get(); // 블록킹 처리된다.
  } catch (Exception ex) {
  
  }
  ```

* 

* 그 순간은 블로킹이 되면서 처리량이 저하된다.

* 처리량이 낮아도 되는 경우에만 사용한다.



### (2) Callback 사용

```
producer.send(new ProducerRecord<>("simple","value"),
   new Callback() {
      @Override
      public void onCompletion(RecordMetadata metadata, Exception ex) {
      
      }
   }
);
```

* 전송결과를 callback 으로 받을 수 있고 exception 이발생하면 전송이 실패한 경우이다.
* 블록킹되는 방식이 아니므로 배치가 쌓이지 않는 등의 단점이 사라진다.
* 처리량 저하 없음
* 







# 3. Consumer 실습



https://www.youtube.com/watch?v=5FEE5wVi8uY





## 1)  auto.offset.reset

Consumer Group 을 처음으로 접속을 시도할때 Topic 의 데이터를 가져오기 위한 옵션이다.



* latest : 가장 마지막 offset 부터
* earlist : 가장 처음 offset 부터 가져온다.















## 1) poll method



```
while (true) {
  ConsumerRecords<String, String> records = consumer.poll(500)
  ...
}
```



poll method 가 지정한 시간동안 데이터가 들어오는 것을 기다린다.

설정값은 500ms 이므로 0.5 초동안 데이터가 유입되는 것을 기다리고 이후 아래 코드를 실행한다.







## 1) commit 

Auto Commit 과 Mannual Commit 에 대해서 알아보자.



### (1) AutoCommit

```
ConsumerConfig.enable_auto_commit_config = true
ConsumerConfig.auto_commit_interval_ms_config = 60000   ; 60초마다 자동으로 commit 수행한다.
```

일정 간격으로 자동 commit



테스트 시나리오

1) Program 을 수행시켜서 Consum 하자.
2) poll() 60초이전에 Program Stop을 눌러버리자.
3) 다시 해당 consumer 를 기동
4) 그럼 초기에 Consume 했던 데이터가 다시 읽어 진다.
5) Commit 되지않았으므로 당연히 두번 읽어진다.
6) 중복처리될 가능성이 있음



특징

* enable.auto.commit = true 가 기본 옵션이다.

* 속도가 빠름
* 중복 또는 유실이 발생할 수 있음
  * 중복/유실을 허용하지 않는 곳에서는 절대 사용금지
  * 



### (2) 수동커밋

enable.auto.commit=false

* commitSync()
  * ConsumerRecord 처리순서를보장함
  * 속도가느리다.
  * consumer 가 읽어드린 마지막 offset을 커밋한다.
  * Map 을 통해 오프셋 지정 커밋 가능

* commitAsync()
  * 동기 커밋 보다 빠름
  * 중복 발생가능
    * 이전offset 보다 이후 offset 이 먼저 커밋되는 경우
  * 처리순서 보장 못함



테스트시나리오

* commitSync() 옵션으로 프로그램 수행
* 프로듀서1~5,  consumer 1-5 수행
* 프로그램 중지
* 프로듀서 전송 6~10
* 프로그램 수행
* 6~10 확인









## 2) concurrency 옵션



## 3) graceful shutdown 방법





Stop 을



## 9) 데이터 유실에 대한 대처







